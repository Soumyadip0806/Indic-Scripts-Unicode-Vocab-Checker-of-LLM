{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTT6MWoYdKPJ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "\n",
        "def login_to_huggingface(api_token):\n",
        "    login(api_token)\n",
        "\n",
        "\n",
        "# Function to initialize the tokenizer and get vocabulary details\n",
        "def initialize_tokenizer(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    vocab_list = list(tokenizer.get_vocab().keys())\n",
        "    vocab_size = len(vocab_list)\n",
        "    print(f\"Model: {model_name}, Vocabulary Size: {vocab_size}\")\n",
        "    return vocab_list, vocab_size\n",
        "\n",
        "\n",
        "\n",
        "def extract_vocab_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return set(line.strip() for line in file.readlines())\n",
        "\n",
        "\n",
        "\n",
        "def process_language_files(directory, vocab_list, vocab_size, model_name):\n",
        "    results = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vocab = extract_vocab_from_file(file_path)\n",
        "            unicode_size = len(vocab)\n",
        "\n",
        "            # Calculate matching and non-matching characters\n",
        "            matching_chars = vocab.intersection(vocab_list)\n",
        "            non_matching_chars = vocab.difference(vocab_list)\n",
        "\n",
        "\n",
        "            language_name = filename.split('.')[0]\n",
        "            results.append({\n",
        "                'modelname': model_name,\n",
        "                'language_name': language_name,\n",
        "                'unicode_size': unicode_size,\n",
        "                'matching_chars': len(matching_chars),\n",
        "                'non_matching_chars': len(non_matching_chars),\n",
        "                'vocab_size': vocab_size,\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "def export_to_csv(results, output_file):\n",
        "    header = ['modelname', 'language_name', 'unicode_size', 'matching_chars', 'non_matching_chars', 'vocab_size']\n",
        "\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "    print(f\"Results have been exported to {output_file}\")\n",
        "\n",
        "\n",
        "def get_output_csv_filename(model_name):\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    return f\"{model_short_name}_vocab_results.csv\"\n",
        "\n",
        "\n",
        "# Main pipeline\n",
        "def main():\n",
        "    # Hugging Face API token and model name\n",
        "    api_token = \"\"\n",
        "    model_name = \"numind/NuExtract-1.5\"\n",
        "\n",
        "\n",
        "    login_to_huggingface(api_token)     # Login to Hugging Face\n",
        "\n",
        "    vocab_list, vocab_size = initialize_tokenizer(model_name)\n",
        "\n",
        "\n",
        "    directory_path = \"/content/drive/MyDrive/Vocab\"       # Directory containing language unicode files\n",
        "    output_csv_file = \"NuExtract-1.5_vocab_results.csv\"\n",
        "    output_csv_file = get_output_csv_filename(model_name)\n",
        "\n",
        "\n",
        "    results = process_language_files(directory_path, vocab_list, vocab_size, model_name)\n",
        "    export_to_csv(results, output_csv_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
